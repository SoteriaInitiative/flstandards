{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6675f99e-268a-4732-bf3f-b6d7da11703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Deactivate all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ad52de-9e31-49de-8450-23483163ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from google.cloud import storage\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Custom weighted loss function\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    loss = K.binary_crossentropy(y_true, y_pred)  \n",
    "    fraud_weight = tf.ones_like(y_true) * 1.0  # Shape: (batch_size, 1)\n",
    "    non_fraud_weight = tf.ones_like(y_true) * 0.01    \n",
    "    weight = tf.where(tf.equal(y_true, 1), fraud_weight, non_fraud_weight)\n",
    "    return K.mean(loss * weight)\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Initialize Google Cloud Storage client\n",
    "def init_gcp_client():\n",
    "    client = storage.Client()\n",
    "    return client\n",
    "\n",
    "gcp_client = init_gcp_client()\n",
    "\n",
    "# Load transactions from Google Cloud Storage\n",
    "def load_transactions(bucket_name, file_name):\n",
    "    try:\n",
    "        bucket = gcp_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "        transactions = json.loads(blob.download_as_text())\n",
    "        return transactions\n",
    "    except Exception as e:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070e816f-c4e9-4082-abed-02ad777abd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def create_model(input_dim):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    hidden = tf.keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=weighted_loss, metrics=[\"AUC\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7487e454-2e02-434e-b9d5-fd0ddd140a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\", \"federated-learning\")\n",
    "NUM_ROUNDS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de174052-2bc7-4340-a5ca-6d8251b1b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "TRANSACTIONS_FILE = f\"Bank_{BANK_ID}_transactions.json\"\n",
    "transactions = load_transactions(GCS_BUCKET_NAME, TRANSACTIONS_FILE)\n",
    "#transactions[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1df9187c-e0e3-4d05-b9cf-a08e741bb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(bank_id):\n",
    "    TRANSACTIONS_FILE = f\"Bank_{bank_id}_transactions.json\"\n",
    "    transactions = load_transactions(GCS_BUCKET_NAME, TRANSACTIONS_FILE)\n",
    "    df = pd.json_normalize(transactions, sep=\"_\")\n",
    "    \n",
    "    # Define possible party type-role combinations\n",
    "    POSSIBLE_PARTY_COMBINATIONS = [\n",
    "        (\"individual\", \"UBO\"),\n",
    "        (\"entity\", \"UBO\"),\n",
    "    ]\n",
    "    party_columns = [f\"party_{ptype}_{prole}\" for ptype, prole in POSSIBLE_PARTY_COMBINATIONS]\n",
    "    \n",
    "    # Function to count occurrences based on predefined values\n",
    "    def count_party_combinations(parties):\n",
    "        counts = {col: 0 for col in party_columns}\n",
    "        for party in parties:\n",
    "            col_name = f\"party_{party.get('party_type')}_{party.get('party_role')}\"\n",
    "            if col_name in counts:\n",
    "                counts[col_name] += 1\n",
    "        return counts\n",
    "    \n",
    "    # Apply function to transactions\n",
    "    party_data = [count_party_combinations(tx.get(\"Transaction\", {}).get(\"account\", {}).get(\"parties\", [])) for tx in transactions]\n",
    "    df_parties = pd.DataFrame(party_data).reindex(columns=party_columns, fill_value=0)\n",
    "    df = pd.concat([df, df_parties], axis=1)\n",
    "    \n",
    "    # Define predefined transaction beneficiary values\n",
    "    POSSIBLE_BENEFICIARIES = [f\"P{i}\" for i in range(1, 11)]\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    encoder = OneHotEncoder(categories=[POSSIBLE_BENEFICIARIES], drop=\"first\", sparse_output=False, handle_unknown=\"ignore\")\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X = df[[\n",
    "        \"Transaction_transaction_type\", \n",
    "        \"Transaction_currency_amount\", \n",
    "        \"Transaction_account_country_code\", \n",
    "        \"Transaction_transaction_beneficiary_country_code\",\n",
    "        \"Transaction_transaction_beneficiary\"\n",
    "    ] + party_columns]  # Include new party count features\n",
    "    \n",
    "    y = df[\"Transaction_local_label\"]\n",
    "    \n",
    "    # Create a ColumnTransformer to apply OHE to categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"transaction_type\", encoder, [\"Transaction_transaction_type\"]),\n",
    "            (\"account_country\", encoder, [\"Transaction_account_country_code\"]),\n",
    "            (\"beneficiary_country\", encoder, [\"Transaction_transaction_beneficiary_country_code\"]),\n",
    "            (\"beneficiary\", encoder, [\"Transaction_transaction_beneficiary\"]),\n",
    "            (\"currency_amount\", \"passthrough\", [\"Transaction_currency_amount\"]),\n",
    "            (\"party_counts\", \"passthrough\", party_columns),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Apply the transformer\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train_local, y_test_local, train_indices, test_indices = train_test_split(\n",
    "        X_processed, y, df.index, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Extract global labels for the test set\n",
    "    y_test_global = df.loc[test_indices, \"Transaction_global_label\"].values\n",
    "    \n",
    "    # Scale currency amount (assuming min-max scaling)\n",
    "    X_train[:, -len(party_columns)-1] = (X_train[:, -len(party_columns)-1] - 10) / (50000 - 10)\n",
    "    X_test[:, -len(party_columns)-1] = (X_test[:, -len(party_columns)-1] - 10) / (50000 - 10)\n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_train, y_test_local = y_train_local.values, y_test_local.values\n",
    "    \n",
    "    model = create_model(X_train.shape[1])\n",
    "    history = model.fit(X_train, y_train, epochs=30*NUM_ROUNDS, batch_size=64, verbose=0)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    local_train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    local_auc = roc_auc_score(y_test_local, y_test_pred)\n",
    "    global_auc = roc_auc_score(y_test_global, y_test_pred)\n",
    "    print(f\"\"\"\n",
    "            AUC of detecting 1 scenario on the training set: {local_train_auc:0.4f},\n",
    "            AUC of detecting 1 scenario on the test set: {local_auc:0.4f},\n",
    "            AUC of detecting 4 scenari on the test set : {global_auc:0.4f}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64af5c28-b517-4c3b-ba4d-84179cd65937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "            AUC of detecting 1 scenario on the training set: 0.7405,\n",
      "            AUC of detecting 1 scenario on the test set: 0.7072,\n",
      "            AUC of detecting 4 scenari on the test set : 0.5735\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "evaluate_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c01cb1d9-c1f8-4398-aa04-4eea870f161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "            AUC of detecting 1 scenario on the training set: 0.5388,\n",
      "            AUC of detecting 1 scenario on the test set: 0.5229,\n",
      "            AUC of detecting 4 scenari on the test set : 0.5738\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "evaluate_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f37ad453-0162-4eb5-8565-6392f5612aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "            AUC of detecting 1 scenario on the training set: 0.9998,\n",
      "            AUC of detecting 1 scenario on the test set: 0.9999,\n",
      "            AUC of detecting 4 scenari on the test set : 0.6689\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "evaluate_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fca7682-244d-49fd-a0d9-71e3ef3baa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "            AUC of detecting 1 scenario on the training set: 1.0000,\n",
      "            AUC of detecting 1 scenario on the test set: 1.0000,\n",
      "            AUC of detecting 4 scenari on the test set : 0.6977\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "evaluate_model(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a4852f-baae-4bab-8b12-fbc273fee126",
   "metadata": {},
   "source": [
    "## Results from Federated Learning after 1 epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d83331-3820-4963-a9c5-613693a37649",
   "metadata": {},
   "source": [
    "### Training Results:\n",
    "- (4000, {'local_train_auc': 0.5619037130440171})\n",
    "- (4000, {'local_train_auc': 0.9998835463760644})\n",
    "- (4000, {'local_train_auc': 0.7388128731215261})\n",
    "- (4000, {'local_train_auc': 1.0})\n",
    "\n",
    "**Aggregated Local Training AUC**: 0.8251500331354019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82fff4-f3cb-4710-afdc-fb7e94e8f960",
   "metadata": {},
   "source": [
    "### Testing Results:\n",
    "- (1000, {'global_auc': 0.8739103111555133, 'local_auc': 0.9191768029882196})\n",
    "- (1000, {'global_auc': 0.8814535884198805, 'local_auc': 0.4971177981437161})\n",
    "- (1000, {'global_auc': 0.8660674121200437, 'local_auc': 0.8510115388920523})\n",
    "- (1000, {'global_auc': 0.8820727895957254, 'local_auc': 0.4941996480082418})\n",
    "\n",
    "**Aggregated local test AUC**: 0.6904  \n",
    "**Aggregated global test AUC**: 0.8759"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9d656-198f-4cfc-991b-076925aa5636",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671db0e1-c7f0-4f7a-94e5-73dd7ac01c78",
   "metadata": {},
   "source": [
    "| Bank - Detection Focus        | Local Model for 1 Scenario | Local Model for 4 Scenari | Federated Learning for 1 Scenario | Federated Learning for 4 Scenari |\n",
    "|------------------------------|-------------------------------|----------------------------------------|--------------------------------------|-------|\n",
    "| Bank 1 - Large Cash Deposits  | AUC = 0.7072                  | AUC = 0.5735                           | AUC = 0.9191                            | AUC = **0.8739** |\n",
    "| Bank 2 – High-Risk Transactions| AUC = 0.5229                  | AUC = 0.5738                           | AUC = 0.4971                            | AUC = **0.8814** |\n",
    "| Bank 3 - Many UBOs            | AUC = 0.9999                  | AUC = 0.6689                           | AUC =  0.8510                            | AUC = **0.8660** |\n",
    "| Bank 4 - Watchlist Entities   | AUC = 1.0000                  | AUC = 0.6977                           | AUC = 0.4941                            | AUC = **0.8820** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e6e9e-9192-4f63-b5ca-c379d764edc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
